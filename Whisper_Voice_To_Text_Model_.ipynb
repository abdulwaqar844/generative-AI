{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIKjrdPBsaiR7cuqVMhEBL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulwaqar844/generative-AI/blob/main/Whisper_Voice_To_Text_Model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpBB5UzLT3hP",
        "outputId": "2808f3c6-5847-4504-9a36-06d8a670720a"
      },
      "source": [
        "!pip install openai-whisper\n",
        "!pip install pytube\n",
        "import whisper\n",
        "import pytube\n",
        "model = whisper.load_model(\"medium.en\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801356 sha256=770d47ed531c8fa4f91d4b97db26bf919c453b587d23cf7988c428b5109549bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20231117 tiktoken-0.6.0\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video = 'https://www.youtube.com/watch?v=pePAAGfh-IU'\n",
        "data = pytube.YouTube(video)\n",
        "audio = data.streams.get_audio_only()\n",
        "audio = audio.download()\n",
        "result = model.transcribe(audio)\n",
        "print(result[\"text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEpSG1C86V1X",
        "outputId": "731e504f-8950-40ac-a1df-6f790118a40c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " If you have a use case for generative AI, how do you decide on which foundation model to pick to run it? With the huge number of foundation models out there, it's not an easy question. Different models are trained on different data and have different parameter counts, and picking the wrong model can have severe unwanted impacts, like biases originating from the training data, or hallucinations that are just plain wrong. Now, one approach is to just pick the largest, most massive model out there to execute every task. The largest models have huge parameter counts and are usually pretty good generalists, but with large models come costs. Costs of compute, costs of complexity, and costs of variability. So, often the better approach is to pick the right size model for the specific use case you have. So, let me propose to you an AI model selection framework. It has six pretty simple stages. Let's take a look at what they are, and then give some examples of how this might work. Now, stage one, that is to clearly articulate your use case. What exactly are you planning to use generative AI for? From there, you'll list some of the model options available to you. Perhaps there are already a subset of foundation models running that you have access to. With a short list of models, you'll next want to identify each model's size, performance, costs, risks, and deployment methods. Next, evaluate those model characteristics for your specific use case. Run some tests, that's the next stage. Testing options based on your previously identified use case and deployment needs, and then finally choose the option that provides the most value. So, let's put this framework to the test. Now, my use case, we're going to say that is a use case for text generation. I need the AI to write personalized emails for my awesome marketing campaign. That's stage one. Now, my organization is already using two foundation models for other things, so I'll evaluate those. First of all, we've got Llama, two, and specifically the Llama 270 model. That's a fairly large model, 70 billion parameters. It's from Meta. I know it's quite good at some text generation use cases. Then there's also Granite that we have deployed. Granite is a smaller general purpose model, and that's from IBM, and I know there is a 13 billion parameter model that I've heard does quite well with text generation as well. So, those are the models I'm going to evaluate, Llama, two, and Granite. Next, we need to evaluate model size, performance, and risks, and a good place to start here is with the model card. The model card might tell us if the model has been trained on data specifically for our purposes. Pre-trained foundation models are fine-tuned for specific use cases, such as sentiment analysis or document summarization, or maybe text generation. That's important to know because if a model is pre-trained on a use case close to ours, it may perform better when processing our prompts and enable us to use zero-shot prompting to obtain our desired results. That means we can simply ask the model to perform tasks without having to provide multiple completed examples first. Now, when it comes to evaluating model performance for our use case, we can consider three factors. The first factor that we would consider is accuracy. Now, accuracy denotes how close the generated output is to the desired output, and it can be measured objectively and repeatedly by choosing evaluation metrics that are relevant to your use cases. So, for example, if your use case related to text translation, the BLEU, that's the bilingual evaluation understudy benchmark, can be used to indicate the quality of the generated translations. Now, the second factor relates to reliability of the model. Now, that's a function of several factors, actually, such as consistency, explainability, and trustworthiness, as well as how well a model avoids toxicity like hate speech. Reliability comes down to trust, and trust is built through transparency and traceability of the training data and accuracy and reliability of the output. And then the third factor, that is speed, and specifically we're saying how quickly does the user get a response to a submitted prompt. Now, speed and accuracy are often a trade-off here. Larger models may be slower, but perhaps deliver a more accurate answer, or then again, maybe the smaller model is faster and has minimal differences in accuracy to the larger model. It really comes down to finding the sweet spot between performance, speed, and cost. A smaller, less expensive model may not offer performance or accuracy metrics on par with an expensive one, but it would still be preferable over the latter if you consider any additional benefits the model might deliver, like lower latency and greater transparency into the model inputs and outputs. The way to find out is to simply select the model that's likely to deliver the desired output, and well, test it. Test that model with your prompts to see if it works, and then assess the model performance and quality of the output using metrics. Now, I've mentioned deployment in passing, so a quick word on that as a decision factor. We need to evaluate where and how we want the model and data to be deployed. So let's say that we're leaning towards Llama 2 as our chosen model based on our testing. Right, cool. Llama 2, that's an open source model, and we could inference with it on a public cloud. So we've got a public cloud already out here. It's got our LLM of choice in it, which is Llama 2. We could just inference to that. But if we decide we want to fine tune the model with our own enterprise data, we might need to deploy it on prem. So this is where we have our own version of Llama 2, and we are going to provide fine tuning to it. Now, deploying on premise gives you greater control and more security benefits compared to a public cloud environment, but it's an expensive proposition, especially when factoring model size and compute power, including the number of GPUs it takes to run a single large language model. Now, everything we've discussed here is tied to a specific use case, but of course it's quite likely that any given organization will have multiple use cases, and as we run through this AI model selection framework, we might find that each use case is better suited to a different foundation model. That's called a multi-model approach. Essentially, not all AI models are the same, and neither are your use cases. And this framework might be just what you need to pair the models and the use cases together to find a winning combination of both.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w9AiF1xqBG-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import pytube\n",
        "model = whisper.load_model(\"medium\")\n",
        "video = 'https://www.youtube.com/watch?v=pePAAGfh-IU'\n",
        "data = pytube.YouTube(video)\n",
        "audio = data.streams.get_audio_only()\n",
        "audio = audio.download()\n",
        "# print( audio.split('/')[-1])\n",
        "# result = model.transcribe(audio)\n",
        "\n",
        "# load audio and pad/trim it to fit 30 seconds\n",
        "audio = whisper.load_audio(audio)\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "# make log-Mel spectrogram and move to the same device as the model\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "# detect the spoken language\n",
        "_, probs = model.detect_language(mel)\n",
        "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
        "\n",
        "# decode the audio\n",
        "options = whisper.DecodingOptions()\n",
        "result = whisper.decode(model, mel, options)\n",
        "\n",
        "# print the recognized text\n",
        "print(result.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI180eQh9Z40",
        "outputId": "7cd7569e-8f6f-4610-c4d3-808081c0063b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: en\n",
            "If you have a use case for generative AI, how do you decide on which foundation model to pick to run it? With the huge number of foundation models out there, it's not an easy question. Different models are trained on different data and have different parameter counts, and picking the wrong model can have severe unwanted impacts, like biases originating from the training data, or hallucinations that are just plain wrong.\n"
          ]
        }
      ]
    }
  ]
}